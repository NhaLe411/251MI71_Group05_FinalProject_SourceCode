{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NhaLe411/251MI71_Group05_FinalProject_SourceCode/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import io\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# ==== Detect relevant columns ====\n",
        "def detect_columns(df):\n",
        "    col_map = {'text': None, 'rating': None, 'date': None}\n",
        "    for col in df.columns:\n",
        "        col_l = col.lower()\n",
        "        if not col_map['text'] and any(k in col_l for k in ['review', 'comment', 'text', 'content']):\n",
        "            col_map['text'] = col\n",
        "        if not col_map['rating'] and any(k in col_l for k in ['rating', 'score', 'stars']):\n",
        "            col_map['rating'] = col\n",
        "        if not col_map['date'] and col_l in ['traveldate', 'season_tag']:\n",
        "            col_map['date'] = col\n",
        "        if not col_map['date'] and any(k in col_l for k in ['date', 'time', 'created']):\n",
        "            col_map['date'] = col\n",
        "    return col_map\n",
        "\n",
        "# ==== Text cleaning, tokenizing ====\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'http[s]?://\\S+', ' ', text)\n",
        "    text = re.sub(r'www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\\'-]', ' ', text)\n",
        "    text = re.sub(r'\\d+(?!\\s*(star|rating|/\\d))', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower().strip()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update({'hotel', 'room', 'stay', 'place', 'time', 'day', 'night', 'would', 'could'})\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [t for t in tokens if t not in stop_words and len(t) > 2 and t.isalpha()]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(tokens):\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "# ==== Sentiment & NPS ====\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "def analyze_sentiment(text):\n",
        "    vader_score = analyzer.polarity_scores(text)['compound']\n",
        "    blob_score = TextBlob(text).sentiment.polarity\n",
        "    score = vader_score * 0.7 + blob_score * 0.3\n",
        "    label = 'positive' if score >= 0.1 else 'negative' if score <= -0.1 else 'neutral'\n",
        "    return pd.Series([label, score, abs(score)])\n",
        "\n",
        "def estimate_rating(label, score, conf):\n",
        "    if label == 'positive':\n",
        "        rating = 8.0 + (score * 2)\n",
        "    elif label == 'negative':\n",
        "        rating = 3.0 + (score + 1.0) * 2\n",
        "    else:\n",
        "        rating = 6.0 + (score * 2)\n",
        "    return max(1, min(10, rating * (0.7 + conf * 0.3)))\n",
        "\n",
        "def get_nps_category(rating):\n",
        "    if rating >= 9: return 'Promoter'\n",
        "    elif rating >= 7: return 'Passive'\n",
        "    else: return 'Detractor'\n",
        "\n",
        "def get_season(date):\n",
        "    if pd.isna(date): return 'Unknown'\n",
        "    month = date.month\n",
        "    if month in [12, 1, 2]: return 'Winter'\n",
        "    elif month in [3, 4, 5]: return 'Spring'\n",
        "    elif month in [6, 7, 8]: return 'Summer'\n",
        "    elif month in [9, 10, 11]: return 'Fall'\n",
        "\n",
        "# ==== Upload & Run ====\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "if filename.endswith('.csv'):\n",
        "    df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "else:\n",
        "    df = pd.read_excel(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "# Detect columns\n",
        "col_map = detect_columns(df)\n",
        "text_col = col_map['text']\n",
        "rating_col = col_map['rating']\n",
        "date_col = col_map['date']\n",
        "\n",
        "df = df[df[text_col].notna() & (df[text_col] != '')].copy()\n",
        "\n",
        "# Preprocess\n",
        "df['cleaned_text'] = df[text_col].apply(clean_text)\n",
        "df['tokens'] = df['cleaned_text'].apply(tokenize)\n",
        "df['lemmatized'] = df['tokens'].apply(lemmatize)\n",
        "df['processed_text'] = df['lemmatized'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Sentiment\n",
        "df[['sentiment_label', 'sentiment_score', 'sentiment_confidence']] = df['cleaned_text'].apply(analyze_sentiment)\n",
        "\n",
        "# Rating\n",
        "if rating_col:\n",
        "    df['final_rating'] = df[rating_col]\n",
        "else:\n",
        "    df['final_rating'] = df.apply(lambda row: estimate_rating(row['sentiment_label'], row['sentiment_score'], row['sentiment_confidence']), axis=1)\n",
        "\n",
        "# NPS\n",
        "df['nps_category'] = df['final_rating'].apply(get_nps_category)\n",
        "\n",
        "# Season (d√πng c·ªôt season_tag n·∫øu c√≥)\n",
        "if 'season_tag' in df.columns:\n",
        "    df['season'] = df['season_tag'].astype(str).str.strip().str.title()\n",
        "else:\n",
        "    if date_col:\n",
        "        df['parsed_date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        df['season'] = df['parsed_date'].apply(get_season)\n",
        "    else:\n",
        "        df['season'] = 'Unknown'\n",
        "\n",
        "# Keywords\n",
        "all_tokens = [t for sub in df['lemmatized'] for t in sub]\n",
        "freq = Counter(all_tokens)\n",
        "common_keywords = ', '.join([w for w, c in freq.most_common(15)])\n",
        "df['top_keywords'] = common_keywords\n",
        "\n",
        "# Analyze seasonal trends\n",
        "print(\"üìä Seasonal Rating Summary:\")\n",
        "season_stats = []\n",
        "\n",
        "for season_name, group in df.groupby('season'):\n",
        "    avg_rating = group['final_rating'].mean()\n",
        "    total = len(group)\n",
        "    pos_pct = (group['sentiment_label'] == 'positive').sum() / total * 100\n",
        "    neg_pct = (group['sentiment_label'] == 'negative').sum() / total * 100\n",
        "    promoters = (group['nps_category'] == 'Promoter').sum()\n",
        "    detractors = (group['nps_category'] == 'Detractor').sum()\n",
        "    nps = ((promoters - detractors) / total) * 100\n",
        "    season_stats.append([season_name, round(avg_rating, 2), f\"{pos_pct:.1f}%\", f\"{neg_pct:.1f}%\", round(nps)])\n",
        "\n",
        "season_df = pd.DataFrame(season_stats, columns=['Season', 'Avg Rating', '% Positive', '% Negative', 'NPS'])\n",
        "print(season_df.to_string(index=False))\n",
        "\n",
        "# Export\n",
        "output_filename = f\"hotel_review_processed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "columns_to_export = [text_col, 'cleaned_text', 'processed_text', 'sentiment_label',\n",
        "                     'sentiment_score', 'sentiment_confidence', 'final_rating',\n",
        "                     'nps_category', 'season']\n",
        "\n",
        "if date_col: columns_to_export.insert(1, date_col)\n",
        "df[columns_to_export].to_csv(output_filename, index=False)\n",
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "7mQOmHPaQxMv",
        "outputId": "1cf202c8-bf5a-41aa-a65d-eece3da34e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vaderSentiment'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2828767425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e2ccaad",
        "outputId": "55bc2eeb-df55-46bb-aa58-ae7778f7fc87"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    }
  ]
}