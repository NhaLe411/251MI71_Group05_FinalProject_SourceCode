# -*- coding: utf-8 -*-
"""TF-IDF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j-uBKIVywmU2MzUwUSMEAlesy75CQaky
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
import pandas as pd

# Bước 1: Tải file CSV từ máy tính
uploaded = files.upload()

# Bước 2: Lấy đúng tên file vừa upload
filename = list(uploaded.keys())[0]

# Bước 3: Đọc file thử với nhiều encoding
encodings = ['utf-8-sig', 'latin1', 'ISO-8859-1']

for enc in encodings:
    try:
        df = pd.read_csv(filename, encoding=enc)
        print(f" Đọc file thành công với encoding = {enc}")
        break
    except Exception as e:
        print(f"Thử với encoding = {enc} thất bại: {e}")

# Bước 4: Hiển thị thông tin dữ liệu
print("\n5 dòng đầu tiên của dữ liệu:")
print(df.head())

print("\nThông tin dữ liệu:")
print(df.info())

print(f"\nSố dòng: {df.shape[0]}")
print(f"Các cột: {df.columns.tolist()}")

if 'sentiment' in df.columns:
    print("\nPhân bố cảm xúc:")
    print(df['sentiment'].value_counts())
else:
    print("\nKhông thấy cột 'sentiment' — hãy xác nhận lại tên cột nhãn trong file CSV.")

# Chuẩn hoá tên cột về dạng yêu cầu bài: text, sentiment
df = df.rename(columns={'Comments': 'text', 'Sentiment': 'sentiment'})

# Chuẩn hoá định dạng nhãn và văn bản
df['sentiment'] = df['sentiment'].astype(str).str.strip().str.lower()
df['text'] = df['text'].astype(str).str.strip()

# Kiểm tra 5 dòng đầu sau chuẩn hoá
print(df.head())

# Thống kê phân bố nhãn
print("\nPhân bố nhãn sentiment:")
print(df['sentiment'].value_counts())

# Đảm bảo chỉ còn đúng 2 cột yêu cầu
df = df[['text', 'sentiment']]
print("\nCác cột cuối cùng:", df.columns.tolist(), "| Số dòng:", len(df))

!pip install -q nltk spacy
!python -m spacy download en_core_web_sm -q

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

print("NLTK ready. Stopwords size:", len(stop_words))

# Sao lưu ban đầu để so sánh
df_raw = df.copy()

n_before = len(df)
df = df.dropna(subset=['text'])
df = df.drop_duplicates(subset=['text'])

print("Rows before:", n_before)
print("Rows after dropna+dedup:", len(df))
print(df[['text','sentiment']].head(3))

df['text_norm'] = (
    df['text']
    .astype(str)
    .str.lower()
    .str.replace(r'\s+', ' ', regex=True)
    .str.strip()
)

print(df[['text','text_norm']].head(5))

# Chỉ giữ a-z và khoảng trắng
df['text_alpha'] = df['text_norm'].str.replace(r'[^a-z\s]', ' ', regex=True)
# Nén khoảng trắng lần nữa
df['text_alpha'] = df['text_alpha'].str.replace(r'\s+', ' ', regex=True).str.strip()

print(df[['text_norm','text_alpha']].head(5))

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

import nltk

df['tokens'] = df['text_alpha'].apply(nltk.word_tokenize)
print(df[['text_alpha','tokens']].head(5))

def remove_stopwords(tokens):
    return [w for w in tokens if w not in stop_words]

df['tokens_nostop'] = df['tokens'].apply(remove_stopwords)
print(df[['tokens','tokens_nostop']].head(5))

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(w) for w in tokens]

df['tokens_lem'] = df['tokens_nostop'].apply(lemmatize_tokens)
print(df[['tokens_nostop','tokens_lem']].head(5))

df['clean_text'] = df['tokens_lem'].apply(lambda toks: " ".join(toks))

n_before = len(df)
df = df[df['clean_text'].str.split().apply(len) >= 5].copy()
n_after = len(df)

print("Rows before length filter:", n_before)
print("Rows after length filter (>=5 words):", n_after)
print(df[['text','clean_text']].head(10))

# Giữ lại đúng 3 cột hữu ích cho bước kế: text gốc, clean_text và nhãn
df_prep = df[['text','clean_text','sentiment']].reset_index(drop=True)

print("Final preprocessed shape:", df_prep.shape)
print(df_prep.sample(5, random_state=42))

df_prep.to_csv("preprocessed_dataset.csv", index=False)
print("Saved file: preprocessed_dataset.csv")

from google.colab import files
files.download("preprocessed_dataset.csv")

!pip install -q scikit-learn vaderSentiment

import pandas as pd
import os

if "df_prep" not in globals():
    assert os.path.exists("preprocessed_dataset.csv"), "Không tìm thấy preprocessed_dataset.csv. Hãy chạy lại Phần 2 để tạo file này."
    df_prep = pd.read_csv("preprocessed_dataset.csv")

print("Shape:", df_prep.shape)
print(df_prep.head(3))

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import json
from scipy import sparse

max_feats = 1000
tfidf = TfidfVectorizer(max_features=max_feats)
X_tfidf = tfidf.fit_transform(df_prep["clean_text"])

print("TF-IDF shape:", X_tfidf.shape)

# Xem một vài đặc trưng đầu tiên
feature_names = tfidf.get_feature_names_out()
print("Sample features:", feature_names[:20])

# Lưu ma trận thưa và vocab
sparse.save_npz("tfidf_features.npz", X_tfidf)
with open("tfidf_vocabulary.json", "w") as f:
    json.dump({w:int(i) for i,w in enumerate(feature_names)}, f)

# Xuất một mẫu nhỏ 10 dòng để minh hoạ trong báo cáo
sample_rows = min(10, X_tfidf.shape[0])
sample = X_tfidf[:sample_rows].toarray()
sample_df = pd.DataFrame(sample, columns=feature_names)
sample_df.insert(0, "sentiment", df_prep["sentiment"].iloc[:sample_rows].values)
sample_df.to_csv("tfidf_sample_10rows.csv", index=False)

print("Đã lưu: tfidf_features.npz, tfidf_vocabulary.json, tfidf_sample_10rows.csv")

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()
scores = df_prep["clean_text"].apply(analyzer.polarity_scores)

# Chuyển list of dicts -> DataFrame
scores_df = pd.DataFrame(list(scores))
scores_df.columns = [f"vader_{c}" for c in scores_df.columns]  # vader_neg, vader_neu, vader_pos, vader_compound

# Ghép vào dữ liệu
df_features = pd.concat([df_prep.reset_index(drop=True), scores_df.reset_index(drop=True)], axis=1)

print("VADER columns:", scores_df.columns.tolist())
print(df_features[["clean_text","vader_neg","vader_neu","vader_pos","vader_compound"]].head(5))

# Lưu riêng bảng điểm VADER và bảng hợp nhất
scores_df.to_csv("vader_scores.csv", index=False)
df_features.to_csv("merged_features.csv", index=False)

print("Đã lưu: vader_scores.csv, merged_features.csv")

print("TF-IDF shape:", X_tfidf.shape)
print("VADER shape:", scores_df.shape)
print("Merged shape:", df_features.shape)

print("\nVí dụ 3 dòng TF-IDF (từ khóa đầu tiên):")
print(pd.DataFrame(X_tfidf[:3].toarray(), columns=feature_names)[feature_names[:10]])

print("\nVí dụ 5 dòng VADER:")
print(df_features[["clean_text","vader_neg","vader_neu","vader_pos","vader_compound"]].head(5))

from google.colab import files

# Các file kết quả cần nộp
files_to_download = [
    "preprocessed_dataset.csv",
    "tfidf_features.npz",
    "tfidf_vocabulary.json",
    "tfidf_sample_10rows.csv",
    "vader_scores.csv",
    "merged_features.csv"
]

for f in files_to_download:
    try:
        files.download(f)
    except Exception as e:
        print(f"Lỗi khi tải {f}: {e}")

from google.colab import drive
drive.mount('/content/drive')